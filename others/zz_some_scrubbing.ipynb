{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import json\n",
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Count': '13032',\n",
       "  'ExcerptPostId': '2667',\n",
       "  'Id': '1',\n",
       "  'TagName': 'mysql',\n",
       "  'WikiPostId': '2666'},\n",
       " {'Count': '1492',\n",
       "  'ExcerptPostId': '3131',\n",
       "  'Id': '2',\n",
       "  'TagName': 'innodb',\n",
       "  'WikiPostId': '3130'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_rdd = sc.textFile(\"./data/results/Tags/part-00000\") \\\n",
    "    .map(lambda x:json.loads(x))\n",
    "tags_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mysql',\n",
       " 'innodb',\n",
       " 'myisam',\n",
       " 'schema',\n",
       " 'nosql',\n",
       " 'rdbms',\n",
       " 'postgresql',\n",
       " 'replication',\n",
       " 'mariadb',\n",
       " 'mysql-5',\n",
       " 'mysqldump',\n",
       " 'sqlite',\n",
       " 'trigger',\n",
       " 'embedded',\n",
       " 'sql',\n",
       " 'performance',\n",
       " 'slow-log',\n",
       " 'benchmark',\n",
       " 'index',\n",
       " 'sql-server-2005',\n",
       " 'sql-server-2008',\n",
       " 'tree',\n",
       " 'document-oriented',\n",
       " 'mongodb',\n",
       " 'google-app-engine',\n",
       " 'null',\n",
       " 'empty-string',\n",
       " 'erd',\n",
       " 'process',\n",
       " 'source-control',\n",
       " 'normalization',\n",
       " 'oracle',\n",
       " 'monitoring',\n",
       " 'tools',\n",
       " 'vendor-support',\n",
       " 'authentication',\n",
       " 'security',\n",
       " 'join',\n",
       " 'foreign-key',\n",
       " 'phpmyadmin',\n",
       " 'xml',\n",
       " 'ms-access',\n",
       " 'performance-testing',\n",
       " 'data-warehouse',\n",
       " 'database-design',\n",
       " 'sql-server',\n",
       " 'deadlock',\n",
       " 'sql-injection',\n",
       " 'audit',\n",
       " 'delete',\n",
       " 'authorization',\n",
       " 'etl',\n",
       " 'constraint',\n",
       " 'testing',\n",
       " 'backup',\n",
       " 'regular-expression',\n",
       " 'db2',\n",
       " 'windows',\n",
       " 'database-link',\n",
       " 'query',\n",
       " 'wallet',\n",
       " 'migration',\n",
       " 'best-practices',\n",
       " 'stored-procedures',\n",
       " 'optimization',\n",
       " 'storage-engine',\n",
       " 'career',\n",
       " 'management',\n",
       " 'learning',\n",
       " 'syntax',\n",
       " 'primary-key',\n",
       " 'aggregate',\n",
       " 'oracle-11g-r2',\n",
       " 'data-pages',\n",
       " 'clustered-index',\n",
       " 'collation',\n",
       " 'uniqueidentifier',\n",
       " 'express-edition',\n",
       " 'maintenance',\n",
       " 'features',\n",
       " 'style',\n",
       " 'version-control',\n",
       " 'oracle-11g',\n",
       " 'datatypes',\n",
       " 'derby',\n",
       " 'database-recommendation',\n",
       " 'instance',\n",
       " 'ssas',\n",
       " 'linux',\n",
       " 'shrink',\n",
       " 'flashback',\n",
       " 'datapump',\n",
       " 'export',\n",
       " 'logs',\n",
       " 'documentation',\n",
       " 'oracle-10g',\n",
       " 'full-text-search',\n",
       " 'hierarchical',\n",
       " 'permissions',\n",
       " 'varchar',\n",
       " 'mysql-workbench',\n",
       " 'sqlplus',\n",
       " 'feature-comparison',\n",
       " 'insert',\n",
       " 'sql-server-2000',\n",
       " 'profiler',\n",
       " 'terminology',\n",
       " 'sybase',\n",
       " 'freetds',\n",
       " 'isql',\n",
       " 'storage',\n",
       " 'locked-objects',\n",
       " 'cte',\n",
       " 'memory',\n",
       " 'odbc',\n",
       " 'unixodbc',\n",
       " 'concurrency',\n",
       " 'ssms',\n",
       " 'order-by',\n",
       " 'database-tuning',\n",
       " 'parallelism',\n",
       " 'my.cnf',\n",
       " 'plsql',\n",
       " 'orm',\n",
       " 'determinism',\n",
       " 'architecture',\n",
       " 'dbcc',\n",
       " 'entity-framework',\n",
       " 'ubuntu',\n",
       " 'client',\n",
       " 'statistics',\n",
       " 'bigtable',\n",
       " 'interview-question',\n",
       " 'pivot',\n",
       " 'rman',\n",
       " 'ssrs',\n",
       " 'parse',\n",
       " 'gae-datastore',\n",
       " 'dump',\n",
       " 'dml',\n",
       " 'jdbc',\n",
       " 'role',\n",
       " 'intellisense',\n",
       " 'clustering',\n",
       " 'max-connections',\n",
       " 'ado.net',\n",
       " 'sharepoint',\n",
       " 'hints',\n",
       " 'pgadmin',\n",
       " 'auto-increment',\n",
       " 'psql',\n",
       " 'command-line',\n",
       " 'bcp',\n",
       " 'bulk',\n",
       " 'ssis',\n",
       " 'filegroups',\n",
       " 'disk-structures',\n",
       " 'visual-studio',\n",
       " 'deployment',\n",
       " 'bulkcopy',\n",
       " 'powershell',\n",
       " 'database-agnostic',\n",
       " 'business-intelligence',\n",
       " 'duplication',\n",
       " 'usertypes',\n",
       " 'metadata',\n",
       " 'random',\n",
       " 'openldap',\n",
       " 'access-control',\n",
       " 'sequence',\n",
       " 'naming-convention',\n",
       " 'scripting',\n",
       " 'dynamic-sql',\n",
       " 't-sql',\n",
       " 'mirroring',\n",
       " 'facttable',\n",
       " 'execution-plan',\n",
       " 'sql-server-2008-r2',\n",
       " 'pgcrypto',\n",
       " 'encryption',\n",
       " 'installation',\n",
       " 'view',\n",
       " 'prepared-statement',\n",
       " 'perfmon',\n",
       " 'dmv',\n",
       " 'partitioning',\n",
       " 'subquery',\n",
       " 'database-size',\n",
       " 'size',\n",
       " 'cache',\n",
       " 'count',\n",
       " 'web-server',\n",
       " 'license',\n",
       " 'transaction',\n",
       " 'configuration',\n",
       " 'oracle-grid-control',\n",
       " 'php',\n",
       " 'tablespaces',\n",
       " 'extended-events',\n",
       " 'olap']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_rdd.map(lambda x:x['TagName']).take(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### idea: classification of posts without tags based on posts with tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def standardize_tags(col):\n",
    "    tag_dict = {\n",
    "        \"innodb\": \"mysql\",\n",
    "#         \"myisam\": \"mysql\",\n",
    "#         \"mysql-5\": \"mysql\",\n",
    "#         \"mysql-5.6\": \"mysql\",\n",
    "#         \"mysqldump\": \"mysql\",\n",
    "#         \"sql-server-2005\": \"sql-server\",\n",
    "#         \"sql-server-2014\": \"sql-server\",\n",
    "#         \"sql-server-2012\": \"sql-server\",\n",
    "#         \"sql-server-2008-r2\": \"sql-server\",\n",
    "#         \"sql-server-2008\": \"sql-server\",\n",
    "#         \"ssms\": \"sql-server\",\n",
    "#         \"ssis\": \"sql-server\",\n",
    "#         \"t-sql\": \"sql-server\",\n",
    "#         \"postgresql-9.3\": \"postgres\",\n",
    "#         \"postgresql\": \"postgres\",\n",
    "#         \"oracle-11g-r2\": \"oracle\",\n",
    "#         \"oracle-11g\": \"oracle\"\n",
    "        }\n",
    "    c=col(col)\n",
    "    for i in tag_dict:\n",
    "        r1=regexp_replace(c, i, tag_dict[i])\n",
    "#        r2=func.regexp_replace(r1,'2','_II_')\n",
    "    return r1.alias(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regexp_replace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def standardize_tags(tag):\n",
    "    r1=regexp_replace(tag, 'innodb', 'mysql')\n",
    "    #r2=regex_replace(r1, 'myisam','mysql')\n",
    "    return r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mysql',\n",
       " 'innodb',\n",
       " 'myisam',\n",
       " 'schema',\n",
       " 'nosql',\n",
       " 'rdbms',\n",
       " 'postgresql',\n",
       " 'replication',\n",
       " 'mariadb',\n",
       " 'mysql-5']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_rdd.map(lambda x: x['TagName']).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 37, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 174, in main\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 169, in process\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Progra~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-69-bc54948d8854>\", line 1, in <lambda>\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 1494, in regexp_replace\n    jc = sc._jvm.functions.regexp_replace(_to_java_column(str), pattern, replacement)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 174, in main\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 169, in process\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Progra~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-69-bc54948d8854>\", line 1, in <lambda>\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 1494, in regexp_replace\n    jc = sc._jvm.functions.regexp_replace(_to_java_column(str), pattern, replacement)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-bc54948d8854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtags_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TagName'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mregexp_replace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'innodb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mysql'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Progra~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Progra~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Progra~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Progra~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Progra~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 37, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 174, in main\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 169, in process\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Progra~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-69-bc54948d8854>\", line 1, in <lambda>\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 1494, in regexp_replace\n    jc = sc._jvm.functions.regexp_replace(_to_java_column(str), pattern, replacement)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 174, in main\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 169, in process\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Progra~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1338, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-69-bc54948d8854>\", line 1, in <lambda>\n  File \"C:\\PROGRA~1\\Tools\\spark-2.1.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 1494, in regexp_replace\n    jc = sc._jvm.functions.regexp_replace(_to_java_column(str), pattern, replacement)\nAttributeError: 'NoneType' object has no attribute '_jvm'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "tags_rdd.map(lambda x: x['TagName']) \\\n",
    "    .map(lambda tag: regexp_replace(tag, 'innodb', 'mysql')).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.select(df[\"TagName\"], df[\"Count\"].cast('int')) \\\n",
    ".filter(df[\"Count\"] > 300).sort(col(\"Count\").desc()).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.withColumn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regexp_replace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-badd6d22fb45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m [7,8,9]]\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mflat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'flat' is not defined"
     ]
    }
   ],
   "source": [
    "l=[[1,2,3],\n",
    "[4,5,6],\n",
    "[7,8,9]]\n",
    "flat(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `df.map` not found.\n"
     ]
    }
   ],
   "source": [
    "df.map?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
